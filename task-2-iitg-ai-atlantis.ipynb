{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":126867,"databundleVersionId":15071216,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-01T03:59:17.724486Z","iopub.execute_input":"2026-01-01T03:59:17.724831Z","iopub.status.idle":"2026-01-01T03:59:18.075411Z","shell.execute_reply.started":"2026-01-01T03:59:17.724801Z","shell.execute_reply":"2026-01-01T03:59:18.074582Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# EDA\n**We start with our EDA**\nfirst we will import all the modules and load up our data set onto a dataframe","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T03:59:23.540118Z","iopub.execute_input":"2026-01-01T03:59:23.541012Z","iopub.status.idle":"2026-01-01T03:59:24.632301Z","shell.execute_reply.started":"2026-01-01T03:59:23.540978Z","shell.execute_reply":"2026-01-01T03:59:24.631274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/iitg-ai-recruitment-2025-beyond-the-box/atlantis_citizens_final.csv\")\n\nprint(df.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T03:59:25.209060Z","iopub.execute_input":"2026-01-01T03:59:25.209511Z","iopub.status.idle":"2026-01-01T03:59:25.313468Z","shell.execute_reply.started":"2026-01-01T03:59:25.209482Z","shell.execute_reply":"2026-01-01T03:59:25.312630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#we will also run a small check \nprint(df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T03:59:28.980786Z","iopub.execute_input":"2026-01-01T03:59:28.981127Z","iopub.status.idle":"2026-01-01T03:59:28.995588Z","shell.execute_reply.started":"2026-01-01T03:59:28.981098Z","shell.execute_reply":"2026-01-01T03:59:28.994701Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"right now we see a number of data that has null data meaning its empty we soon understand some stuff that do not hold much meaning like citizenship id and also data that holds meaning we are not gonna just erase rows that are empty but we will impute them with what we know after.","metadata":{}},{"cell_type":"code","source":"#lets start by cleaning some unnecesaary columns\ncolumns_to_drop = [\"Citizen_ID\", \"Bio_Hash\"]\ndf = df.drop(columns_to_drop, axis = 1).copy()\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T03:59:33.909274Z","iopub.execute_input":"2026-01-01T03:59:33.910446Z","iopub.status.idle":"2026-01-01T03:59:33.932434Z","shell.execute_reply.started":"2026-01-01T03:59:33.910411Z","shell.execute_reply":"2026-01-01T03:59:33.931266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.dtypes#checking to see if each data type is the way we want it to be","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T03:59:36.305424Z","iopub.execute_input":"2026-01-01T03:59:36.306197Z","iopub.status.idle":"2026-01-01T03:59:36.313618Z","shell.execute_reply.started":"2026-01-01T03:59:36.306160Z","shell.execute_reply":"2026-01-01T03:59:36.312679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.describe())\ndf.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T03:59:38.824209Z","iopub.execute_input":"2026-01-01T03:59:38.825266Z","iopub.status.idle":"2026-01-01T03:59:38.846592Z","shell.execute_reply.started":"2026-01-01T03:59:38.825229Z","shell.execute_reply":"2026-01-01T03:59:38.845563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"Vehicle_Owned\"].value_counts().plot(\n    kind=\"bar\",\n    xlabel=\"Type of car\",\n    ylabel=\"Count\",\n    title=\"Vehicle Ownership Distribution\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T03:59:41.016317Z","iopub.execute_input":"2026-01-01T03:59:41.016661Z","iopub.status.idle":"2026-01-01T03:59:41.334393Z","shell.execute_reply.started":"2026-01-01T03:59:41.016635Z","shell.execute_reply":"2026-01-01T03:59:41.333417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#we want to establish connection between each feature and the occupation and further solidify our reasoning\n#we start with non numeric data\n#as such we start with our knowledge to check if by occupation affects the cars owned\n\noccupations = df[\"Occupation\"].unique()\nfor occ in occupations:\n    subset = df[df[\"Occupation\"] == occ]\n    subset[\"Vehicle_Owned\"].value_counts().plot(kind=\"bar\")\n    plt.xlabel(\"Type of car\")\n    plt.ylabel(\"Count\")\n    plt.title(f\"Car types owned for {occ}\")\n    plt.xticks(rotation=45)\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T03:59:45.728799Z","iopub.execute_input":"2026-01-01T03:59:45.729697Z","iopub.status.idle":"2026-01-01T03:59:46.382133Z","shell.execute_reply.started":"2026-01-01T03:59:45.729665Z","shell.execute_reply":"2026-01-01T03:59:46.381239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.crosstab(df[\"Occupation\"], df[\"Vehicle_Owned\"], normalize=\"index\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T17:56:03.667233Z","iopub.execute_input":"2025-12-31T17:56:03.667600Z","iopub.status.idle":"2025-12-31T17:56:03.690238Z","shell.execute_reply.started":"2025-12-31T17:56:03.667572Z","shell.execute_reply":"2025-12-31T17:56:03.689023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ct = pd.crosstab(df[\"Occupation\"], df[\"Vehicle_Owned\"], normalize=\"index\")\nplt.figure(figsize=(6,6))\nsns.heatmap(ct, annot=True, cmap=\"Blues\")\nplt.title(\"Proportion of Vehicle Types per Occupation\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T17:56:03.691629Z","iopub.execute_input":"2025-12-31T17:56:03.691980Z","iopub.status.idle":"2025-12-31T17:56:03.976264Z","shell.execute_reply.started":"2025-12-31T17:56:03.691953Z","shell.execute_reply":"2025-12-31T17:56:03.974987Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We notice a very important trend the relative proportions of an occupation owning a certain vehicle differs even though it will be harder to notice in absolute numbers this provides a very good insight proving that the vehicle owned plays a crucial role in the occupation deciding factors","metadata":{}},{"cell_type":"code","source":"#we now move on to diet type\noccupations = df[\"Occupation\"].unique()\nfor occ in occupations:\n    subset = df[df[\"Occupation\"] == occ]\n    subset[\"Diet_Type\"].value_counts().plot(kind=\"bar\")\n    plt.xlabel(\"Diet type\")\n    plt.ylabel(\"Count\")\n    plt.title(f\"Diet types for {occ}\")\n    plt.xticks(rotation=45)\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T17:56:03.977502Z","iopub.execute_input":"2025-12-31T17:56:03.977788Z","iopub.status.idle":"2025-12-31T17:56:04.586739Z","shell.execute_reply.started":"2025-12-31T17:56:03.977763Z","shell.execute_reply":"2025-12-31T17:56:04.585499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ct = pd.crosstab(df[\"Occupation\"], df[\"Diet_Type\"], normalize=\"index\")\nplt.figure(figsize=(10,6))\nsns.heatmap(ct, annot=True, cmap=\"Blues\")\nplt.title(\"Proportion of Diet Types per Occupation\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T17:56:04.588335Z","iopub.execute_input":"2025-12-31T17:56:04.588818Z","iopub.status.idle":"2025-12-31T17:56:04.808333Z","shell.execute_reply.started":"2025-12-31T17:56:04.588779Z","shell.execute_reply":"2025-12-31T17:56:04.807294Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The key takeaway from this heat map is we see that all the proportions lie around the 0.32 to 0.34 mark which shows little to no variation so this feature on its own carries very little significance we might later check its significance when paired with other features but for now it is a low impact feature","metadata":{}},{"cell_type":"code","source":"#district names being studied \nct = pd.crosstab(df[\"Occupation\"], df[\"District_Name\"], normalize=\"index\")\nplt.figure(figsize=(12,6))\nsns.heatmap(ct, annot=True, cmap=\"Purples\")\nplt.title(\"Proportion of Districts per Occupation\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T17:56:04.809896Z","iopub.execute_input":"2025-12-31T17:56:04.810215Z","iopub.status.idle":"2025-12-31T17:56:05.062755Z","shell.execute_reply.started":"2025-12-31T17:56:04.810186Z","shell.execute_reply":"2025-12-31T17:56:05.061667Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A very impactful feature as the heat maps show. Proportions are widely seperated and show clear distinctions one of the strongest distinguishing features we have encountered so far","metadata":{}},{"cell_type":"code","source":"#lets run the similar heat map testing through work district\nct = pd.crosstab(df[\"Occupation\"], df[\"Work_District\"], normalize=\"index\")\nplt.figure(figsize=(12,6))\nsns.heatmap(ct, annot=True, cmap=\"Oranges\")\nplt.title(\"Proportion of Work Districts per Occupation\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T17:56:05.064129Z","iopub.execute_input":"2025-12-31T17:56:05.064572Z","iopub.status.idle":"2025-12-31T17:56:05.328417Z","shell.execute_reply.started":"2025-12-31T17:56:05.064543Z","shell.execute_reply":"2025-12-31T17:56:05.327520Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Like the previous one this shows very great distinctions with nicely space proportions and is another useful one in our distinguishing features","metadata":{}},{"cell_type":"markdown","source":"**We are done with out catergorical data now we move on to the numeric ones the thing is with numeric ones we have to make sure that the plot diagrams are nicely picked otherwise we might misjudge it we will try something like boxplot**","metadata":{}},{"cell_type":"code","source":"#starting with House_size_sq_ft and plotting the boxplot of the data\n\nplt.figure(figsize=(10,6))\nsns.boxplot(x=\"Occupation\", y=\"House_Size_sq_ft\", data=df)\nplt.title(\"House Size Distribution by Occupation\")\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T17:56:05.329902Z","iopub.execute_input":"2025-12-31T17:56:05.330701Z","iopub.status.idle":"2025-12-31T17:56:05.578397Z","shell.execute_reply.started":"2025-12-31T17:56:05.330664Z","shell.execute_reply":"2025-12-31T17:56:05.577354Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is a very good sign for us as we see that this numeric data aligns very well and somewhat matches with intuition(merchants being wealthy having a higher house size on average while the fishers being poorer have a smaller house), this tells us that our data on house sizes can very well be used for classification","metadata":{}},{"cell_type":"code","source":"#moving on to wealth index \nplt.figure(figsize=(10,6))\nsns.boxplot(x=\"Occupation\", y=\"Wealth_Index\", data=df)\nplt.title(\"Wealth Index Distribution by Occupation\")\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T17:56:05.579742Z","iopub.execute_input":"2025-12-31T17:56:05.580860Z","iopub.status.idle":"2025-12-31T17:56:06.108985Z","shell.execute_reply.started":"2025-12-31T17:56:05.580815Z","shell.execute_reply":"2025-12-31T17:56:06.107846Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The data is skewed a lot by the outliers, a quick but not so efficient fix would be to go with log of the wealth index and re plotting our box Plots","metadata":{}},{"cell_type":"code","source":"df[\"Wealth_Index_log\"] = np.log1p(df[\"Wealth_Index\"])\n\nplt.figure(figsize=(10,6))\nsns.boxplot(x=\"Occupation\", y=\"Wealth_Index_log\", data=df)\nplt.title(\"Log-Transformed Wealth Index Distribution by Occupation\")\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:00:35.592497Z","iopub.execute_input":"2026-01-01T04:00:35.593602Z","iopub.status.idle":"2026-01-01T04:00:35.957637Z","shell.execute_reply.started":"2026-01-01T04:00:35.593563Z","shell.execute_reply":"2026-01-01T04:00:35.956444Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we see clear distinct seperation in the log values of wealth index proving it to be useful feature for classification and so we will include that in our data\n\nThis was our first action under **\"Feature Engineering\"**","metadata":{}},{"cell_type":"code","source":"#going with the Life_expectance\nplt.figure(figsize=(10,6))\nsns.boxplot(x=\"Occupation\", y=\"Life_Expectancy\", data=df)\nplt.title(\"Life Expectancy Distribution by Occupation\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T17:56:06.358935Z","iopub.execute_input":"2025-12-31T17:56:06.359329Z","iopub.status.idle":"2025-12-31T17:56:06.582337Z","shell.execute_reply.started":"2025-12-31T17:56:06.359265Z","shell.execute_reply":"2025-12-31T17:56:06.581331Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we see that our life expectancy data is also a very important column as even though with outliers our data is quite well behaved and shows proper distinguishing features","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.pairplot(df, \n             vars=[\"House_Size_sq_ft\", \"Wealth_Index_log\", \"Life_Expectancy\"], \n             hue=\"Occupation\", \n             diag_kind=\"kde\", \n             plot_kws={\"alpha\":0.6})\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T17:56:06.583511Z","iopub.execute_input":"2025-12-31T17:56:06.583890Z","iopub.status.idle":"2025-12-31T17:56:14.278522Z","shell.execute_reply.started":"2025-12-31T17:56:06.583864Z","shell.execute_reply":"2025-12-31T17:56:14.277469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df  = df.drop(\"Wealth_Index\", axis = 1).copy()\nplt.figure(figsize=(10,8))\nsns.heatmap(df.corr(numeric_only=True), \n            annot=True, \n            cmap=\"coolwarm\", \n            center=0)\nplt.title(\"Correlation Heatmap of Numeric Features\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:01:20.263281Z","iopub.execute_input":"2026-01-01T04:01:20.263681Z","iopub.status.idle":"2026-01-01T04:01:20.465192Z","shell.execute_reply.started":"2026-01-01T04:01:20.263653Z","shell.execute_reply":"2026-01-01T04:01:20.464155Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Our analysis of the three numeric data gave a lot of information, the house sizes andwealth index log is showing a lot of correlation and so does the other numeric data","metadata":{}},{"cell_type":"code","source":"df[[\"House_Size_sq_ft\", \"Wealth_Index_log\", \"Life_Expectancy\"]] = (\n    df.groupby(\"Occupation\")[[\"House_Size_sq_ft\", \"Wealth_Index_log\", \"Life_Expectancy\"]]\n      .transform(lambda x: x.fillna(x.median()))\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:02:37.019455Z","iopub.execute_input":"2026-01-01T04:02:37.019859Z","iopub.status.idle":"2026-01-01T04:02:37.045949Z","shell.execute_reply.started":"2026-01-01T04:02:37.019827Z","shell.execute_reply":"2026-01-01T04:02:37.044610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:02:40.131692Z","iopub.execute_input":"2026-01-01T04:02:40.132046Z","iopub.status.idle":"2026-01-01T04:02:40.145193Z","shell.execute_reply.started":"2026-01-01T04:02:40.132009Z","shell.execute_reply":"2026-01-01T04:02:40.144409Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# The First Model (Random Forest)\nour data preprocessing is done for now we have included the features that are very effective and picked out ones which are useful now it is time for us to see how our data analysis can perform when we put a model to use it.\nWe start with a basic Random Forest Classfier model to see how it performs as a baseline\nit will be using F1 macro score as its evaluation metric","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\n\n# Features and target\nX = df.drop(columns=[\"Occupation\"])\ny = df[\"Occupation\"]\n\n# Identifing our categorical and numeric columns\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns\nnumeric_cols = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n\n# getting preprocessing done\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols)\n    ]\n)\n\nclf = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", RandomForestClassifier(n_estimators=200, random_state=42))\n])\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\n# Fitting our model with the train set \nclf.fit(X_train, y_train)\n\n# Predicting and evaluateing\ny_pred = clf.predict(X_test)\nprint(\"F1-macro:\", f1_score(y_test, y_pred, average=\"macro\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T17:56:14.534796Z","iopub.execute_input":"2025-12-31T17:56:14.535156Z","iopub.status.idle":"2025-12-31T17:56:18.958314Z","shell.execute_reply.started":"2025-12-31T17:56:14.535127Z","shell.execute_reply":"2025-12-31T17:56:18.957173Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Our Test results**\nwe see that from our test results that our macro score has crossed the 0.5 mark without much effort and is sitting close to 0.58\nwe shall now see how much we can improve on this model and learn what we can from this before moving to a more advanced model\nWe start by introducing a change : making the class weight balanced so that all classes are equally distributed in the training data","metadata":{}},{"cell_type":"markdown","source":"**test 2 with our random forest**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import f1_score,accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\nX = df.drop(columns=[\"Occupation\"])\ny = df[\"Occupation\"]\n\n\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns\nnumeric_cols = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols)\n    ]\n)\n\n\nclf_balanced = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", RandomForestClassifier(\n        n_estimators=200,\n        random_state=42,\n        class_weight=\"balanced\"   # balancing our class weights so that even minority classes are represented\n    ))\n])\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n\nclf_balanced.fit(X_train, y_train)\ny_pred = clf_balanced.predict(X_test)\nprint(\"F1-macro (balanced):\", f1_score(y_test, y_pred, average=\"macro\"))\nprint(\"accuracy: \", accuracy_score(y_test,y_pred))\nclf=clf_balanced","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T17:56:18.959700Z","iopub.execute_input":"2025-12-31T17:56:18.960079Z","iopub.status.idle":"2025-12-31T17:56:23.606564Z","shell.execute_reply.started":"2025-12-31T17:56:18.960041Z","shell.execute_reply":"2025-12-31T17:56:23.605544Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see this betters our score by a very small fraction but still improves it we shall now improve on it further","metadata":{}},{"cell_type":"markdown","source":"**Test 3 with random forest using K-fold CV**\n\nWith our first model of a Random forest Classifier we are achieving a test result of a F1 Macro score of 58.48% that is giving us a few more doors of opportunity to explore, but we want to make sure that our random forest classifier be maxed out with its capacities before going to other models.\nLets try Hyperparameter tuning.\nWe set up a stratified K fold CV to see how it scores  using still F1 macro as our evaluation matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import make_scorer, f1_score\n\n\nf1_macro = make_scorer(f1_score, average=\"macro\")\n\n\ncv = StratifiedKFold(n_splits= 10, shuffle=True, random_state=42)\n\n\nscores = cross_val_score(clf, X, y, cv=cv, scoring=\"f1_macro\")\n\nprint(\"F1-macro scores per fold:\", scores)\nprint(\"Average F1-macro:\", scores.mean())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T17:56:23.607744Z","iopub.execute_input":"2025-12-31T17:56:23.608114Z","iopub.status.idle":"2025-12-31T17:57:11.370670Z","shell.execute_reply.started":"2025-12-31T17:56:23.608075Z","shell.execute_reply":"2025-12-31T17:57:11.369398Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter tuning for our Random Forest\n","metadata":{}},{"cell_type":"markdown","source":"We see that our model does cross 0.6 mark and is consistendly near that across all the folds and even reaching as high as 0.62.\nWe will now try hyper Parameter tuning using randomized search CV","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import make_scorer, f1_score\nimport numpy as np\n\n\nf1_macro = make_scorer(f1_score, average=\"macro\")\n\n\nparam_dist = {\n    \"model__n_estimators\": [200, 300, 500],\n    \"model__max_depth\": [None, 10, 20, 30],\n    \"model__min_samples_split\": [2, 5, 10],\n    \"model__min_samples_leaf\": [1, 2, 4],\n    \"model__max_features\": [\"sqrt\", \"log2\"],\n    \"model__class_weight\": [None, \"balanced\"]\n}\n\n\nrandom_search = RandomizedSearchCV(\n    clf,                \n    param_distributions=param_dist,\n    n_iter=20,         \n    cv=5,               \n    scoring=\"f1_macro\",\n    random_state=42,\n    n_jobs=-1\n)\n\nrandom_search.fit(X, y)\n\nprint(\"Best parameters:\", random_search.best_params_)\nprint(\"Best CV F1-macro:\", random_search.best_score_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T17:57:11.372124Z","iopub.execute_input":"2025-12-31T17:57:11.372444Z","iopub.status.idle":"2025-12-31T18:00:17.573765Z","shell.execute_reply.started":"2025-12-31T17:57:11.372415Z","shell.execute_reply":"2025-12-31T18:00:17.572585Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Our score is very much near 0.63 and we realise our best parameters for the model","metadata":{}},{"cell_type":"markdown","source":"**Testing with our best found parameters**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nX = df.drop(columns=[\"Occupation\"])\ny = df[\"Occupation\"]\n\n\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns\nnumeric_cols = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols)\n    ]\n)\n\n# Pipeline with tuned hyperparameters\nclf = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", RandomForestClassifier(\n        n_estimators=500,\n        max_depth=10,\n        min_samples_split=5,\n        min_samples_leaf=1,\n        max_features=\"sqrt\",\n        class_weight=\"balanced\",\n        random_state=42,\n        n_jobs=-1\n    ))\n])\n\n# Fit on the full dataset (no split)\nclf.fit(X, y)\nprint(\"training compelte\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T18:00:17.578846Z","iopub.execute_input":"2025-12-31T18:00:17.579319Z","iopub.status.idle":"2025-12-31T18:00:20.826344Z","shell.execute_reply.started":"2025-12-31T18:00:17.579244Z","shell.execute_reply":"2025-12-31T18:00:20.825376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/iitg-ai-recruitment-2025-beyond-the-box/test_atlantis_hidden.csv\")\nprint(df.isna().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T18:00:20.827751Z","iopub.execute_input":"2025-12-31T18:00:20.828588Z","iopub.status.idle":"2025-12-31T18:00:20.854531Z","shell.execute_reply.started":"2025-12-31T18:00:20.828544Z","shell.execute_reply":"2025-12-31T18:00:20.853293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test[\"Wealth_Index_log\"] = np.log1p(df_test[\"Wealth_Index\"])\nids = df_test[\"Citizen_ID\"]\ndf_test = df_test.drop([\"Wealth_Index\", \"Bio_Hash\", \"Citizen_ID\"], axis = 1).copy()\nprint(df_test.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T18:00:20.855492Z","iopub.execute_input":"2025-12-31T18:00:20.855745Z","iopub.status.idle":"2025-12-31T18:00:20.868675Z","shell.execute_reply.started":"2025-12-31T18:00:20.855723Z","shell.execute_reply":"2025-12-31T18:00:20.867835Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# First Submission","metadata":{}},{"cell_type":"code","source":"\noccupation_map = {\n    \"Warrior\": 0,\n    \"Merchant\": 1,\n    \"Fisher\": 2,\n    \"Miner\": 3,\n    \"Scribe\": 4\n}\n\n\ntest_preds = clf.predict(df_test)\n\n\ntest_preds_encoded = [occupation_map[label] for label in test_preds]\n\n\nsubmission = pd.DataFrame({\n    \"Citizen_ID\": ids,              \n    \"Occupation\": test_preds_encoded\n})\n\n\nsubmission.to_csv(\"submission__.csv\", index=False)\nprint(\"Submission file created with encoded Occupation labels.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T18:00:20.869703Z","iopub.execute_input":"2025-12-31T18:00:20.870023Z","iopub.status.idle":"2025-12-31T18:00:21.155301Z","shell.execute_reply.started":"2025-12-31T18:00:20.869996Z","shell.execute_reply":"2025-12-31T18:00:21.154074Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Our model score 00.607 on the scoreboard but we need to push it beyond 0.65 as much as we can**\nwe start by going deeper into our random forest and use Randomized search to find the best parameters and then output the F1 macro score and then train it on our entire dataset and give the output\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n\nX = df.drop(columns=[\"Occupation\"])\ny = df[\"Occupation\"]\n\n\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns\nnumeric_cols = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols)\n    ]\n)\n\n\nclf = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", RandomForestClassifier(\n        n_estimators=1000,\n        min_samples_split=20,\n        min_samples_leaf=1,\n        max_features=0.5,\n        max_depth=30,\n        class_weight=\"balanced\",\n        random_state=42,\n        n_jobs=4\n    ))\n])\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    stratify=y,\n    random_state=42\n)\n\n\nclf.fit(X_train, y_train)\n\n\ntest_preds = clf.predict(X_test)\nprint(\"Test F1-macro:\", f1_score(y_test, test_preds, average=\"macro\", zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T18:09:44.031139Z","iopub.execute_input":"2025-12-31T18:09:44.032071Z","iopub.status.idle":"2025-12-31T18:09:54.433323Z","shell.execute_reply.started":"2025-12-31T18:09:44.032036Z","shell.execute_reply":"2025-12-31T18:09:54.432115Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training on whole dataSet to see if our score on submission betters**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n# Features and target\nX = df.drop(columns=[\"Occupation\"])\ny = df[\"Occupation\"]\n\n\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns\nnumeric_cols = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols)\n    ]\n)\n\n\nclf = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", RandomForestClassifier(\n        n_estimators=1000,\n        min_samples_split=20,\n        min_samples_leaf=1,\n        max_features=0.5,\n        max_depth=30,\n        class_weight=\"balanced\",\n        random_state=42,\n        n_jobs=4\n    ))\n])\n\n\n\nclf.fit(X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T18:20:58.785733Z","iopub.execute_input":"2025-12-31T18:20:58.786217Z","iopub.status.idle":"2025-12-31T18:21:11.106358Z","shell.execute_reply.started":"2025-12-31T18:20:58.786172Z","shell.execute_reply":"2025-12-31T18:21:11.105298Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Submitting after training on whole data frame","metadata":{}},{"cell_type":"code","source":"\noccupation_map = {\n    \"Warrior\": 0,\n    \"Merchant\": 1,\n    \"Fisher\": 2,\n    \"Miner\": 3,\n    \"Scribe\": 4\n}\n\n\ntest_preds = clf.predict(df_test)\n\n\ntest_preds_encoded = [occupation_map[label] for label in test_preds]\n\n\nsubmission = pd.DataFrame({\n    \"Citizen_ID\": ids,              \n    \"Occupation\": test_preds_encoded\n})\n\n\nsubmission.to_csv(\"submission_3.csv\", index=False)\nprint(\"Submission file created with encoded Occupation labels.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T18:24:52.036442Z","iopub.execute_input":"2025-12-31T18:24:52.036796Z","iopub.status.idle":"2025-12-31T18:24:52.541432Z","shell.execute_reply.started":"2025-12-31T18:24:52.036769Z","shell.execute_reply":"2025-12-31T18:24:52.540187Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Test 1:**\nBest parameters: {'model__n_estimators': 1000, 'model__min_samples_split': 20, 'model__min_samples_leaf': 1,\n'model__max_features': 0.5, 'model__max_depth': 30, 'model__class_weight': 'balanced'}\nBest CV F1-macro: 0.6373094017590735\nTest F1-macro: 0.6331419918941843","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n\nX = df.drop(columns=[\"Occupation\"])\ny = df[\"Occupation\"]\n\n\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns.drop(\"Diet_Type\")\nnumeric_cols = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols)\n    ]\n)\n\nclf = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", RandomForestClassifier(\n        n_estimators=1000,\n        min_samples_split=20,\n        min_samples_leaf=1,\n        max_features=0.5,\n        max_depth=30,\n        class_weight=\"balanced\",\n        random_state=42,\n        n_jobs=4\n    ))\n])\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    stratify=y,\n    random_state=42\n)\n\n\nclf.fit(X_train, y_train)\n\n\ntest_preds = clf.predict(X_test)\nprint(\"Test F1-macro:\", f1_score(y_test, test_preds, average=\"macro\", zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T18:10:33.018646Z","iopub.execute_input":"2025-12-31T18:10:33.019013Z","iopub.status.idle":"2025-12-31T18:10:43.338408Z","shell.execute_reply.started":"2025-12-31T18:10:33.018981Z","shell.execute_reply":"2025-12-31T18:10:43.337500Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*We tried dropping the diet type column but as it turns out our model's score has dropped we will try introducing some features before moving on to another model*","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering (The Wealth Per Capita)\n\nwe enlist our first completely engineered feature of our dataset\nthe wealth per capita. we encountered it during our EDA in task 1","metadata":{}},{"cell_type":"code","source":"\ndf[\"Wealth_per_capita\"] = df[\"Wealth_Index_log\"] / (df[\"House_Size_sq_ft\"] + 1)\n\nprint(df.groupby(\"Occupation\")[\"Wealth_per_capita\"].describe())\n\nplt.figure(figsize=(10,6))\nsns.boxplot(x=\"Occupation\", y=\"Wealth_per_capita\", data=df)\nplt.xticks(rotation=45)\nplt.title(\"Wealth per capita distribution across Occupations\")\nplt.show()\n\nmean_values = df.groupby(\"Occupation\")[\"Wealth_per_capita\"].mean()\nmean_values = mean_values / mean_values.sum()\nprint(\"Normalized mean Wealth_per_capita proportions:\\n\", mean_values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:05:39.457388Z","iopub.execute_input":"2026-01-01T04:05:39.458435Z","iopub.status.idle":"2026-01-01T04:05:39.725800Z","shell.execute_reply.started":"2026-01-01T04:05:39.458388Z","shell.execute_reply":"2026-01-01T04:05:39.724928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see that this variation is more distinct and we get much more info out of this feature that the raw ones seperately","metadata":{}},{"cell_type":"code","source":"df[[\"Wealth_Index_log\", \"House_Size_sq_ft\", \"Wealth_per_capita\"]].corr()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:07:50.445032Z","iopub.execute_input":"2026-01-01T04:07:50.445816Z","iopub.status.idle":"2026-01-01T04:07:50.469367Z","shell.execute_reply.started":"2026-01-01T04:07:50.445777Z","shell.execute_reply":"2026-01-01T04:07:50.468281Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we see a quite an appreciable correlation between all the three we shall decide to keep it","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:11:06.727565Z","iopub.execute_input":"2026-01-01T04:11:06.728000Z","iopub.status.idle":"2026-01-01T04:11:06.735895Z","shell.execute_reply.started":"2026-01-01T04:11:06.727964Z","shell.execute_reply":"2026-01-01T04:11:06.734648Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training with our engineered features**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n\nX = df.drop(columns=[\"Occupation\"])\ny = df[\"Occupation\"]\n\n\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns\nnumeric_cols = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols)\n    ]\n)\n\n\nclf = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", RandomForestClassifier(\n        n_estimators=1000,\n        min_samples_split=20,\n        min_samples_leaf=1,\n        max_features=0.5,\n        max_depth=30,\n        class_weight=\"balanced\",\n        random_state=42,\n        n_jobs=4\n    ))\n])\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    stratify=y,\n    random_state=42\n)\n\n\nclf.fit(X_train, y_train)\n\n\ntest_preds = clf.predict(X_test)\nprint(\"Test F1-macro:\", f1_score(y_test, test_preds, average=\"macro\", zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:11:26.967391Z","iopub.execute_input":"2026-01-01T04:11:26.967700Z","iopub.status.idle":"2026-01-01T04:11:41.798895Z","shell.execute_reply.started":"2026-01-01T04:11:26.967674Z","shell.execute_reply":"2026-01-01T04:11:41.797633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_names = clf.named_steps[\"preprocessor\"].get_feature_names_out()\nimportances = clf.named_steps[\"model\"].feature_importances_\nfeat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\nprint(feat_imp.head(20))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:12:39.471988Z","iopub.execute_input":"2026-01-01T04:12:39.472578Z","iopub.status.idle":"2026-01-01T04:12:39.789026Z","shell.execute_reply.started":"2026-01-01T04:12:39.472542Z","shell.execute_reply":"2026-01-01T04:12:39.788134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Experimenting by dropping Vehicle_owned feature**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n\nX = df.drop(columns=[\"Occupation\"])\ny = df[\"Occupation\"]\n\n\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns.drop(\"Vehicle_Owned\")\nnumeric_cols = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols)\n    ]\n)\n\n\nclf = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", RandomForestClassifier(\n        n_estimators=1000,\n        min_samples_split=20,\n        min_samples_leaf=1,\n        max_features=0.5,\n        max_depth=30,\n        class_weight=\"balanced\",\n        random_state=42,\n        n_jobs=4\n    ))\n])\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    stratify=y,\n    random_state=42\n)\n\n\nclf.fit(X_train, y_train)\n\n\ntest_preds = clf.predict(X_test)\nprint(\"Test F1-macro:\", f1_score(y_test, test_preds, average=\"macro\", zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:15:24.155815Z","iopub.execute_input":"2026-01-01T04:15:24.156398Z","iopub.status.idle":"2026-01-01T04:15:36.381614Z","shell.execute_reply.started":"2026-01-01T04:15:24.156313Z","shell.execute_reply":"2026-01-01T04:15:36.380598Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see our model performance more or less unchanged meaning that the lower importance feature \"Vehicle Owned\" was not contributing anything substantial but noise so we shall drop that feature as we move forth","metadata":{}},{"cell_type":"markdown","source":"# Experimenting with Features","metadata":{}},{"cell_type":"markdown","source":"since district name and work district also had an influence on the job and and it greatly varied the diet type i want to experiment using bins like such so that we can find if the model can use that for classifying","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\ndf[\"District_Diet\"] = df[\"District_Name\"] + \"_\" + df[\"Diet_Type\"]\ndf[\"WorkDistrict_Diet\"] = df[\"Work_District\"] + \"_\" + df[\"Diet_Type\"]\n\nX = df.drop(columns=[\"Occupation\"])\ny = df[\"Occupation\"]\n\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns.drop(\"Vehicle_Owned\")\nnumeric_cols = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols)\n    ]\n)\n\nclf = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", RandomForestClassifier(\n        n_estimators=1000,\n        min_samples_split=20,\n        min_samples_leaf=1,\n        max_features=0.5,\n        max_depth=30,\n        class_weight=\"balanced\",\n        random_state=42,\n        n_jobs=4\n    ))\n])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    stratify=y,\n    random_state=42\n)\n\nclf.fit(X_train, y_train)\n\ntest_preds = clf.predict(X_test)\nprint(\"Test F1-macro:\", f1_score(y_test, test_preds, average=\"macro\", zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:18:23.938932Z","iopub.execute_input":"2026-01-01T04:18:23.940068Z","iopub.status.idle":"2026-01-01T04:19:05.553165Z","shell.execute_reply.started":"2026-01-01T04:18:23.940018Z","shell.execute_reply":"2026-01-01T04:19:05.552169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_names = clf.named_steps[\"preprocessor\"].get_feature_names_out()\nimportances = clf.named_steps[\"model\"].feature_importances_\n\nfeat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\nprint(feat_imp.head(30))  # top 30 features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:19:41.331247Z","iopub.execute_input":"2026-01-01T04:19:41.332168Z","iopub.status.idle":"2026-01-01T04:19:41.624771Z","shell.execute_reply.started":"2026-01-01T04:19:41.332133Z","shell.execute_reply":"2026-01-01T04:19:41.623937Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model did not show any improvement with these features and on the importance list they rank very less","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nX = df.drop(columns=[\"Occupation\", \"District_Diet\", \"WorkDistrict_Diet\"])\ny = df[\"Occupation\"]\n\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns.drop(\"Vehicle_Owned\")\nnumeric_cols = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols)\n    ]\n)\n\nclf = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", RandomForestClassifier(\n        n_estimators=1000,\n        min_samples_split=20,\n        min_samples_leaf=1,\n        max_features=0.5,\n        max_depth=30,\n        class_weight=\"balanced\",\n        random_state=42,\n        n_jobs=4\n    ))\n])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    stratify=y,\n    random_state=42\n)\n\nclf.fit(X_train, y_train)\n\ntest_preds = clf.predict(X_test)\nprint(\"Test F1-macro:\", f1_score(y_test, test_preds, average=\"macro\", zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:22:38.344465Z","iopub.execute_input":"2026-01-01T04:22:38.345241Z","iopub.status.idle":"2026-01-01T04:22:50.373458Z","shell.execute_reply.started":"2026-01-01T04:22:38.345203Z","shell.execute_reply":"2026-01-01T04:22:50.372407Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Experimenting this time with our wealth index","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf[\"Wealth_Index_log_bin\"] = pd.qcut(df[\"Wealth_Index_log\"], q=4, labels=[\"low\",\"mid_low\",\"mid_high\",\"high\"])\ndf[\"House_Size_bin\"] = pd.qcut(df[\"House_Size_sq_ft\"], q=4, labels=[\"small\",\"medium_small\",\"medium_large\",\"large\"])\n\nX = df.drop(columns=[\"Occupation\", \"District_Diet\", \"WorkDistrict_Diet\",\"Vehicle_Owned\"])\ny = df[\"Occupation\"]\n\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns\nnumeric_cols = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols)\n    ]\n)\n\nclf = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", RandomForestClassifier(\n        n_estimators=1000,\n        min_samples_split=20,\n        min_samples_leaf=1,\n        max_features=0.5,\n        max_depth=30,\n        class_weight=\"balanced\",\n        random_state=42,\n        n_jobs=4\n    ))\n])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    stratify=y,\n    random_state=42\n)\n\nclf.fit(X_train, y_train)\ntest_preds = clf.predict(X_test)\nprint(\"Test F1-macro:\", f1_score(y_test, test_preds, average=\"macro\", zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:25:51.122409Z","iopub.execute_input":"2026-01-01T04:25:51.123313Z","iopub.status.idle":"2026-01-01T04:26:03.169862Z","shell.execute_reply.started":"2026-01-01T04:25:51.123278Z","shell.execute_reply":"2026-01-01T04:26:03.168566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_names = clf.named_steps[\"preprocessor\"].get_feature_names_out()\nimportances = clf.named_steps[\"model\"].feature_importances_\n\nfeat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\nprint(feat_imp.head(30))  # top 30 features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:26:50.021260Z","iopub.execute_input":"2026-01-01T04:26:50.021723Z","iopub.status.idle":"2026-01-01T04:26:50.318374Z","shell.execute_reply.started":"2026-01-01T04:26:50.021691Z","shell.execute_reply":"2026-01-01T04:26:50.317347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:31:49.821597Z","iopub.execute_input":"2026-01-01T04:31:49.822048Z","iopub.status.idle":"2026-01-01T04:31:49.833910Z","shell.execute_reply.started":"2026-01-01T04:31:49.822015Z","shell.execute_reply":"2026-01-01T04:31:49.833181Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training another model after dropping the experimented features**","metadata":{}},{"cell_type":"code","source":"df = df.drop([\"Vehicle_Owned\", \"District_Diet\", \"WorkDistrict_Diet\", \"Wealth_Index_log_bin\",\"House_Size_bin\"],axis=1)\nprint(df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:33:11.232148Z","iopub.execute_input":"2026-01-01T04:33:11.232861Z","iopub.status.idle":"2026-01-01T04:33:11.241270Z","shell.execute_reply.started":"2026-01-01T04:33:11.232827Z","shell.execute_reply":"2026-01-01T04:33:11.240140Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CatBoost Model\n\nthe lines that follow each codel block contain the the console logs that the catboost model showed","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nX = df.drop(columns=[\"Occupation\"])\ny = df[\"Occupation\"]\n\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    stratify=y,\n    random_state=42\n)\n\nclf = CatBoostClassifier(\n    iterations=1000,\n    depth=8,\n    learning_rate=0.05,\n    loss_function=\"MultiClass\",\n    eval_metric=\"MultiClass\",\n    cat_features=categorical_cols,\n    random_seed=42,\n    verbose=200\n)\n\nclf.fit(X_train, y_train,eval_set = (X_test,y_test))\n\ntest_preds = clf.predict(X_test)\nprint(\"Test F1-macro:\", f1_score(y_test, test_preds, average=\"macro\", zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:44:32.805068Z","iopub.execute_input":"2026-01-01T04:44:32.806269Z","iopub.status.idle":"2026-01-01T04:45:50.507047Z","shell.execute_reply.started":"2026-01-01T04:44:32.806229Z","shell.execute_reply":"2026-01-01T04:45:50.506121Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Test 1\n\n0:\tlearn: 1.5601968\ttotal: 135ms\tremaining: 2m 14s\n200:\tlearn: 0.7812936\ttotal: 14.5s\tremaining: 57.6s\n400:\tlearn: 0.6807556\ttotal: 29.7s\tremaining: 44.4s\n600:\tlearn: 0.6249210\ttotal: 45s\tremaining: 29.9s\n800:\tlearn: 0.5795493\ttotal: 1m\tremaining: 15s\n999:\tlearn: 0.5403172\ttotal: 1m 15s\tremaining: 0us\nTest F1-macro: 0.6517691465783431 \nThis was test 1 with CatBoost and no modifications\n\n\n# Test 2\n0:\tlearn: 1.5601968\ttest: 1.5628883\tbest: 1.5628883 (0)\ttotal: 79ms\tremaining: 1m 18s\n200:\tlearn: 0.7812936\ttest: 0.8650590\tbest: 0.8650590 (200)\ttotal: 14.8s\tremaining: 58.9s\n400:\tlearn: 0.6807556\ttest: 0.8129358\tbest: 0.8129358 (400)\ttotal: 30.4s\tremaining: 45.4s\n600:\tlearn: 0.6249210\ttest: 0.8044889\tbest: 0.8043910 (596)\ttotal: 45.9s\tremaining: 30.5s\n800:\tlearn: 0.5795493\ttest: 0.8023722\tbest: 0.8017557 (700)\ttotal: 1m 1s\tremaining: 15.3s\n999:\tlearn: 0.5403172\ttest: 0.8041424\tbest: 0.8017557 (700)\ttotal: 1m 17s\tremaining: 0us\n\nbestTest = 0.8017556836\nbestIteration = 700\n\nShrink model to first 701 iterations.\nTest F1-macro: 0.6592440471947738\nThis was our test 2 with CatBoost and setting eval_set so that our model does not flatlines when learning and stops when it sees that its performance is falling \n\n","metadata":{}},{"cell_type":"markdown","source":"# Experimenting with class weights\n\nwe tried modifying the class weights so that the minority classes get equal representation we try many different formula for class weight","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nX = df.drop(columns=[\"Occupation\"])\ny = df[\"Occupation\"]\n\nclasses, counts = np.unique(y, return_counts=True)\ntotal = len(y)\n\n\nweights = {cls: total / (len(classes) * count) for cls, count in zip(classes, counts)}\n\n\nclass_weights = [weights[cls] for cls in classes]\n\nprint(\"Class weights:\", dict(zip(classes, class_weights)))\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    stratify=y,\n    random_state=42\n)\n\n\nclf = CatBoostClassifier(\n    iterations=1000,\n    depth=8,\n    learning_rate=0.05,\n    loss_function=\"MultiClass\",\n    cat_features=X.select_dtypes(include=[\"object\"]).columns.tolist(),\n    class_weights=class_weights,\n    random_seed=42,\n    verbose=200\n)\n\nclf.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=100)\n\ntest_preds = clf.predict(X_test)\nprint(\"Test F1-macro:\", f1_score(y_test, test_preds, average=\"macro\", zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T04:56:06.672118Z","iopub.execute_input":"2026-01-01T04:56:06.673031Z","iopub.status.idle":"2026-01-01T04:57:14.299984Z","shell.execute_reply.started":"2026-01-01T04:56:06.672995Z","shell.execute_reply":"2026-01-01T04:57:14.299067Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test 3\nClass weights: {'Fisher': np.float64(1.0045280612244898), 'Merchant': np.float64(0.8911456859971711), 'Miner': np.float64(1.0403566710700132), 'Scribe': np.float64(1.249583498611662), 'Warrior': np.float64(0.8921551968280941)}\n0:\tlearn: 1.5647856\ttest: 1.5670019\tbest: 1.5670019 (0)\ttotal: 42.4ms\tremaining: 42.4s\n200:\tlearn: 0.7888568\ttest: 0.8761359\tbest: 0.8761359 (200)\ttotal: 15.2s\tremaining: 1m\n400:\tlearn: 0.6879808\ttest: 0.8229574\tbest: 0.8229574 (400)\ttotal: 30.9s\tremaining: 46.2s\n600:\tlearn: 0.6336274\ttest: 0.8129771\tbest: 0.8127935 (595)\ttotal: 46.7s\tremaining: 31s\n800:\tlearn: 0.5863063\ttest: 0.8110601\tbest: 0.8100837 (757)\ttotal: 1m 2s\tremaining: 15.6s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.8100836626\nbestIteration = 757\n\nShrink model to first 758 iterations.\nTest F1-macro: 0.6522951194726081\n\nthis test was done with putting class weight inversely proportional to their counts we do notice a slight dip in score","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nX = df.drop(columns=[\"Occupation\"])\ny = df[\"Occupation\"]\n\n\nclasses, counts = np.unique(y, return_counts=True)\ntotal = len(y)\n\n\nweights = {cls: np.sqrt(total/(len(classes)*count)) for cls, count in zip(classes, counts)}\n\n\nclass_weights = [weights[cls] for cls in classes]\n\nprint(\"Class weights:\", dict(zip(classes, class_weights)))\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    stratify=y,\n    random_state=42\n)\n\n\nclf = CatBoostClassifier(\n    iterations=1000,\n    depth=8,\n    learning_rate=0.05,\n    loss_function=\"MultiClass\",\n    cat_features=X.select_dtypes(include=[\"object\"]).columns.tolist(),\n    class_weights=class_weights,\n    random_seed=42,\n    verbose=200\n)\n\nclf.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=100)\n\ntest_preds = clf.predict(X_test)\nprint(\"Test F1-macro:\", f1_score(y_test, test_preds, average=\"macro\", zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T05:00:04.069194Z","iopub.execute_input":"2026-01-01T05:00:04.069748Z","iopub.status.idle":"2026-01-01T05:01:19.021551Z","shell.execute_reply.started":"2026-01-01T05:00:04.069713Z","shell.execute_reply":"2026-01-01T05:01:19.020433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, test_preds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T05:02:22.138188Z","iopub.execute_input":"2026-01-01T05:02:22.139120Z","iopub.status.idle":"2026-01-01T05:02:22.183994Z","shell.execute_reply.started":"2026-01-01T05:02:22.139081Z","shell.execute_reply":"2026-01-01T05:02:22.182799Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test 4\nClass weights: {'Fisher': np.float64(1.002261473481092), 'Merchant': np.float64(0.9440051302811713), 'Miner': np.float64(1.0199787601072943), 'Scribe': np.float64(1.1178477081479667), 'Warrior': np.float64(0.9445396745653907)}\n0:\tlearn: 1.5583713\ttest: 1.5607479\tbest: 1.5607479 (0)\ttotal: 107ms\tremaining: 1m 46s\n200:\tlearn: 0.7904772\ttest: 0.8750976\tbest: 0.8750976 (200)\ttotal: 15.5s\tremaining: 1m 1s\n400:\tlearn: 0.6854620\ttest: 0.8185688\tbest: 0.8185688 (400)\ttotal: 31.3s\tremaining: 46.8s\n600:\tlearn: 0.6250139\ttest: 0.8088275\tbest: 0.8086688 (596)\ttotal: 47.2s\tremaining: 31.4s\n800:\tlearn: 0.5792265\ttest: 0.8071120\tbest: 0.8071081 (799)\ttotal: 1m 3s\tremaining: 15.7s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.8063136285\nbestIteration = 843\n\nShrink model to first 844 iterations.\nTest F1-macro: 0.65816499293084\n\nthis test was done with catboost having sqrt of inverse as the weights from the classwise score we understand the model is not performing good enough for miners and fishers","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, classification_report\n\nX = df.drop(columns=[\"Occupation\"])\ny = df[\"Occupation\"]\n\n\nclasses, counts = np.unique(y, return_counts=True)\ntotal = len(y)\n\n\nweights = {cls: 1.0 for cls in classes}\n\n\nweights[\"Fisher\"] = 1.3   \nweights[\"Miner\"]  = 1.3   \n\n\nclass_weights = [weights[cls] for cls in classes]\n\nprint(\"Selective class weights:\", dict(zip(classes, class_weights)))\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    stratify=y,\n    random_state=42\n)\n\nclf = CatBoostClassifier(\n    iterations=1000,\n    depth=8,\n    learning_rate=0.05,\n    loss_function=\"MultiClass\",\n    cat_features=X.select_dtypes(include=[\"object\"]).columns.tolist(),\n    class_weights=class_weights,\n    random_seed=42,\n    verbose=200\n)\n\nclf.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=100)\n\ntest_preds = clf.predict(X_test)\nprint(\"Test F1-macro:\", f1_score(y_test, test_preds, average=\"macro\", zero_division=0))\nprint(classification_report(y_test, test_preds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T05:05:36.438527Z","iopub.execute_input":"2026-01-01T05:05:36.439538Z","iopub.status.idle":"2026-01-01T05:06:37.568522Z","shell.execute_reply.started":"2026-01-01T05:05:36.439499Z","shell.execute_reply":"2026-01-01T05:06:37.567595Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test 5\nthis test we did it with manually increasing the wieghts of minority classes so their classification is done better , next we try hyper paramter tuning with grid search CV","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostClassifier\n\nparam_grid = {\n    'depth': [6, 8, 10],\n    'learning_rate': [0.03, 0.05],\n    'iterations': [1000, 2000],\n    'l2_leaf_reg': [5, 7]\n}\n\nclf = CatBoostClassifier(\n    loss_function=\"MultiClass\",\n    cat_features=X.select_dtypes(include=[\"object\"]).columns.tolist(),\n    class_weights=class_weights,\n    random_seed=42,\n    verbose=200\n)\n\ngrid = GridSearchCV(\n    clf,\n    param_grid,\n    scoring='f1_macro',\n    cv=2,\n    n_jobs=-1\n)\n\n\ngrid.fit(\n    X_train, y_train,\n    eval_set=(X_test, y_test),\n    early_stopping_rounds=100\n)\n\nprint(\"Best params:\", grid.best_params_)\nprint(\"Best CV F1-macro:\", grid.best_score_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T05:11:08.541046Z","iopub.execute_input":"2026-01-01T05:11:08.541465Z","iopub.status.idle":"2026-01-01T06:45:23.347288Z","shell.execute_reply.started":"2026-01-01T05:11:08.541436Z","shell.execute_reply":"2026-01-01T06:45:23.345863Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**we tried experimenting with a bigger grid but our notebook was crashing and we were getting errors so we tried a smaller grid**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostClassifier\n\n\nparam_grid = {\n    'learning_rate': [0.02, 0.03, 0.04, 0.05],  \n    'iterations': [1000, 1500],                \n    'depth': [8],                               \n    'l2_leaf_reg': [5]                        \n}\n\nclf = CatBoostClassifier(\n    loss_function=\"MultiClass\",\n    cat_features=X.select_dtypes(include=[\"object\"]).columns.tolist(),\n    class_weights=class_weights,  \n    random_seed=42,\n    verbose=200\n)\n\ngrid = GridSearchCV(\n    clf,\n    param_grid,\n    scoring='f1_macro',\n    cv=2,         \n    n_jobs=-1\n)\n\n\ngrid.fit(\n    X_train, y_train,\n    eval_set=(X_test, y_test),\n    early_stopping_rounds=100\n)\n\nprint(\"Best params:\", grid.best_params_)\nprint(\"Best CV F1-macro:\", grid.best_score_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T06:49:48.716667Z","iopub.execute_input":"2026-01-01T06:49:48.717540Z","iopub.status.idle":"2026-01-01T07:07:04.476924Z","shell.execute_reply.started":"2026-01-01T06:49:48.717507Z","shell.execute_reply":"2026-01-01T07:07:04.475819Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Creating another submission with training on full set with our parameters","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom catboost import CatBoostClassifier\n\nX = df.drop(columns=[\"Occupation\"])\ny = df[\"Occupation\"]\n\n\nclasses, counts = np.unique(y, return_counts=True)\n\n\nweights = {cls: 1.0 for cls in classes}\n\n\nweights[\"Fisher\"] = 1.3   \nweights[\"Miner\"]  = 1.3  \n\n\nclass_weights = [weights[cls] for cls in classes]\n\nprint(\"Selective class weights:\", dict(zip(classes, class_weights)))\n\n\nclf = CatBoostClassifier(\n    iterations=1000,\n    depth=8,\n    learning_rate=0.05,\n    loss_function=\"MultiClass\",\n    cat_features=X.select_dtypes(include=[\"object\"]).columns.tolist(),\n    class_weights=class_weights,\n    random_seed=42,\n    verbose=200\n)\n\nclf.fit(X, y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T07:21:47.847470Z","iopub.execute_input":"2026-01-01T07:21:47.848265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we do notice a bit of a problem as the model did crash in between and we lost a bit of code but we will try to replicate where we left\n\n\n\n\nIndex(['Diet_Type', 'District_Name', 'Occupation', 'Wealth_Index', 'House_Size_sq_ft', 'Life_Expectancy', 'Vehicle_Owned', 'Work_District', 'House_Size_log', 'Wealth_Index_log', 'Wealth_per_capita', 'Wealth_per_year'], dtype='object')\n","metadata":{}},{"cell_type":"markdown","source":"We try running a Randomized CV search of this grid and find out the best parameters","metadata":{}},{"cell_type":"markdown","source":"A Little bit of code was lost as the bigger randomized grid search was done we did get the best parameters but it kept crashin when tried to restart we tried setting n_jobs = 4 but it only resulted in a longer process all in all we did boil down to some final best parameters ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"/kaggle/input/iitg-ai-recruitment-2025-beyond-the-box/atlantis_citizens_final.csv\")\n\ndf[\"House_Size_log\"] = np.log1p(df[\"House_Size_sq_ft\"])\ndf[\"Wealth_Index_log\"] = np.log1p(df[\"Wealth_Index\"])\ndf[\"Wealth_per_capita\"] = df[\"Wealth_Index\"] / (df[\"House_Size_sq_ft\"] +  1e-6)\ndf[\"Wealth_per_year\"] = df[\"Wealth_Index\"] / (df[\"Life_Expectancy\"] +  1e-6)\ndf = df.drop([\"Bio_Hash\", \"Citizen_ID\"], axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T15:39:34.826165Z","iopub.execute_input":"2026-01-04T15:39:34.826994Z","iopub.status.idle":"2026-01-04T15:39:35.239027Z","shell.execute_reply.started":"2026-01-04T15:39:34.826949Z","shell.execute_reply":"2026-01-04T15:39:35.238315Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"Running a code on randomized search CV to get recommended parameters for our model\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV \nfrom catboost import CatBoostClassifier \nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nparam_dist = { \n    'learning_rate': [0.02, 0.03, 0.05], \n    'depth': [6, 8, 10], \n    'l2_leaf_reg': [1, 3, 5], \n    'iterations': [800, 1000] \n}\n\n\n\n\nX = df.drop(columns=[\"Occupation\", \"House_Size_sq_ft\"])\ny = df[\"Occupation\"]\n\n\n\nclasses, counts = np.unique(y, return_counts=True)\nweights = {cls: 1.0 for cls in classes}\n\n\nweights[\"Fisher\"] = 1.3   \nweights[\"Miner\"]  = 1.3   \nclass_weights = [weights[cls] for cls in classes]\n\n\ncat_features=X.select_dtypes(include=[\"object\"]).columns.tolist()\n\n\n\n\n\ntotal = len(y)\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    stratify=y,\n    random_state=42\n)\n\n\n\n\n\nclf = CatBoostClassifier( \n    loss_function=\"MultiClass\", \n    cat_features=cat_features, \n    class_weights=class_weights, \n    random_seed=42, \n    verbose=0 )\n\nsearch = RandomizedSearchCV( estimator=clf,\n                            param_distributions=param_dist,\n                            n_iter=20, \n                            scoring='f1_macro', \n                            cv=3, \n                            n_jobs=-1, \n                            random_state=42 \n                           )\nsearch.fit(X_train, y_train) \nprint(\"Best params:\", search.best_params_) \nprint(\"Best CV F1-macro:\", search.best_score_)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T06:51:18.550797Z","iopub.execute_input":"2026-01-04T06:51:18.551102Z","iopub.status.idle":"2026-01-04T06:51:18.557644Z","shell.execute_reply.started":"2026-01-04T06:51:18.551075Z","shell.execute_reply":"2026-01-04T06:51:18.556733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nimport pandas as pd\n\ncat_features = ['Diet_Type','District_Name','Vehicle_Owned','Work_District']\n\nX = df.drop(columns=[\"Occupation\",\"House_Size_sq_ft\"])\ny = df[\"Occupation\"]\n\nclasses = y.unique()\nweights = {cls: 1.0 for cls in classes}\nweights[\"Fisher\"] = 1.3\nweights[\"Miner\"] = 1.3\n\nfinal_clf = CatBoostClassifier(\n    learning_rate=0.03,\n    depth=6,\n    l2_leaf_reg=3,\n    iterations=1000,\n    loss_function=\"MultiClass\",\n    cat_features=cat_features,\n    class_weights=weights,\n    random_seed=42,\n    verbose=200\n)\n\nfinal_clf.fit(X,y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T13:05:20.728467Z","iopub.execute_input":"2026-01-03T13:05:20.728791Z","iopub.status.idle":"2026-01-03T13:06:27.880333Z","shell.execute_reply.started":"2026-01-03T13:05:20.728764Z","shell.execute_reply":"2026-01-03T13:06:27.879522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.columns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T13:11:23.512795Z","iopub.execute_input":"2026-01-03T13:11:23.513883Z","iopub.status.idle":"2026-01-03T13:11:23.519740Z","shell.execute_reply.started":"2026-01-03T13:11:23.513841Z","shell.execute_reply":"2026-01-03T13:11:23.518818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndf_test = pd.read_csv(\"/kaggle/input/iitg-ai-recruitment-2025-beyond-the-box/test_atlantis_hidden.csv\")\n\ndf_test[\"House_Size_log\"] = np.log1p(df_test[\"House_Size_sq_ft\"])\ndf_test[\"Wealth_Index_log\"] = np.log1p(df_test[\"Wealth_Index\"])\ndf_test[\"Wealth_per_capita\"] = df_test[\"Wealth_Index\"] / (df_test[\"House_Size_sq_ft\"] +  1e-6)\ndf_test[\"Wealth_per_year\"] = df_test[\"Wealth_Index\"] / (df_test[\"Life_Expectancy\"] +  1e-6)\ndf_test = df_test.drop([\"Bio_Hash\"], axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T13:09:35.238298Z","iopub.execute_input":"2026-01-03T13:09:35.238737Z","iopub.status.idle":"2026-01-03T13:09:35.268283Z","shell.execute_reply.started":"2026-01-03T13:09:35.238704Z","shell.execute_reply":"2026-01-03T13:09:35.267476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Submission","metadata":{}},{"cell_type":"code","source":"X_test = df_test.drop(columns=[\"Citizen_ID\", \"House_Size_sq_ft\"])\ntest_preds = final_clf.predict(X_test).flatten()\n\noccupation_map = {\n    \"Warrior\": 0,\n    \"Merchant\": 1,\n    \"Fisher\": 2,\n    \"Miner\": 3,\n    \"Scribe\": 4\n}\n\ntest_preds_labels = [occupation_map[p] for p in test_preds]\n\nsubmission = pd.DataFrame({\n    \"Citizen_ID\": df_test[\"Citizen_ID\"],\n    \"Occupation\": test_preds_labels\n})\n\nsubmission.to_csv(\"submission_6.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T13:13:57.372121Z","iopub.execute_input":"2026-01-03T13:13:57.372530Z","iopub.status.idle":"2026-01-03T13:13:57.411143Z","shell.execute_reply.started":"2026-01-03T13:13:57.372497Z","shell.execute_reply":"2026-01-03T13:13:57.410325Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# End\n\nthis notebook ends here with submission_6 which gave us a F1 macro score 0.633 on the leaderboard\nthe overall training was satisfactory.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}